{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minimal_bert_dot_usage_example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng63tDwZSSm5"
      },
      "source": [
        "# Using Our TAS-B Bert_Dot (or BERT Dense Retrieval) Checkpoint\n",
        "\n",
        "We provide a fully retrieval trained (with topic aware and balanced margin sampling: TAS-B) DistilBert-based instance on the HuggingFace model hub here: https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\n",
        "\n",
        "This instance can be used to **re-rank a candidate set** or **directly for a vector index based dense retrieval**. The architecure is a 6-layer DistilBERT, without architecture additions or modifications (we only change the weights during training) - to receive a query/passage representation we pool the CLS vector. \n",
        "\n",
        "If you want to know more about our efficient batch composition procedure and dual supervision for dense retrieval training, check out our paper: https://arxiv.org/abs/2104.06967 ðŸŽ‰\n",
        "\n",
        "This notebook gives you a minimal usage example of downloading our Bert_Dot checkpoint to encode passages and queries to create a dot-product based score of their relevance. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's get started by installing the awesome *transformers* library from HuggingFace:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2WyNOE2R2rW"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqkWDa_jWu7c"
      },
      "source": [
        "The next step is to download our checkpoint and initialize the tokenizer and models:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTYEtziISSDl"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# you can switch the model to the original \"distilbert-base-uncased\" to see that the usage example then breaks and the score ordering is reversed :O\n",
        "#pre_trained_model_name = \"distilbert-base-uncased\"\n",
        "pre_trained_model_name = \"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_name) \n",
        "bert_model = AutoModel.from_pretrained(pre_trained_model_name)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 504/504 [00:00<00:00, 128kB/s]\n",
            "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 262k/262k [00:02<00:00, 91.1kB/s]\n",
            "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 18.7kB/s]\n",
            "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 306/306 [00:00<00:00, 61.9kB/s]\n",
            "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 265M/265M [13:03<00:00, 339kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOGT8YQQX1Ot"
      },
      "source": [
        "Now we are ready to use the model to encode two sample passages and a query:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzt9Ix9UYMLy",
        "outputId": "529e338e-b4e7-4251-cf9b-4363ac8a3ed8"
      },
      "source": [
        "# our relevant example\n",
        "passage1_input = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library for pre-trained language models. We are helping the community work together towards the goal of advancing NLP ðŸ”¥.\",return_tensors=\"pt\")\n",
        "# a non-relevant example\n",
        "passage2_input = tokenizer(\"Hmm I don't like this new movie about transformers that i got from my local library. Those transformers are robots?\",return_tensors=\"pt\")\n",
        "# the user query -> which should give us a better score for the first passage\n",
        "query_input = tokenizer(\"what is the transformers library\",return_tensors=\"pt\")\n",
        "\n",
        "print(\"Passage 1 Tokenized:\",tokenizer.convert_ids_to_tokens(passage1_input[\"input_ids\"][0]))\n",
        "print(\"Passage 2 Tokenized:\",tokenizer.convert_ids_to_tokens(passage2_input[\"input_ids\"][0]))\n",
        "print(\"Query Tokenized:\",tokenizer.convert_ids_to_tokens(query_input[\"input_ids\"][0]))\n",
        "\n",
        "# note how we call the bert model independently between passages and query :)\n",
        "# [0][:,0,:] pools (or selects) the CLS vector from the full output\n",
        "passage1_encoded = bert_model(**passage1_input)[0][:,0,:].squeeze(0)\n",
        "passage2_encoded = bert_model(**passage2_input)[0][:,0,:].squeeze(0)\n",
        "query_encoded    = bert_model(**query_input)[0][:,0,:].squeeze(0)\n",
        "\n",
        "print(\"---\")\n",
        "print(\"Passage Encoded Shape:\",passage1_encoded.shape)\n",
        "print(\"Query Encoded Shape:\",query_encoded.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passage 1 Tokenized: ['[CLS]', 'we', 'are', 'very', 'happy', 'to', 'show', 'you', 'the', '[UNK]', 'transformers', 'library', 'for', 'pre', '-', 'trained', 'language', 'models', '.', 'we', 'are', 'helping', 'the', 'community', 'work', 'together', 'towards', 'the', 'goal', 'of', 'advancing', 'nl', '##p', '[UNK]', '.', '[SEP]']\nPassage 2 Tokenized: ['[CLS]', 'hmm', 'i', 'don', \"'\", 't', 'like', 'this', 'new', 'movie', 'about', 'transformers', 'that', 'i', 'got', 'from', 'my', 'local', 'library', '.', 'those', 'transformers', 'are', 'robots', '?', '[SEP]']\nQuery Tokenized: ['[CLS]', 'what', 'is', 'the', 'transformers', 'library', '[SEP]']\n---\nPassage Encoded Shape: torch.Size([768])\nQuery Encoded Shape: torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_96RCg7Y1cP"
      },
      "source": [
        "Now that we have our encoded vectors, we can generate the score with a simple dot product! \r\n",
        "\r\n",
        "(This can be offloaded to a vector indexing library like Faiss)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzDL1qKDalIR",
        "outputId": "ee6271b2-0da4-4717-8c7e-2730f84475fe"
      },
      "source": [
        "score_for_p1 = query_encoded.dot(passage1_encoded)\n",
        "print(\"Score passage 1 <-> query: \",float(score_for_p1))\n",
        "\n",
        "score_for_p2 = query_encoded.dot(passage2_encoded)\n",
        "print(\"Score passage 2 <-> query: \",float(score_for_p2))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score passage 1 <-> query:  103.33966064453125\nScore passage 2 <-> query:  97.69734954833984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1bY5qB9b-AI"
      },
      "source": [
        "As we see the model gives the first passage a higher score than the second - these scores would now be used to generate a list (if we run this comparison on all passages in our collection or candidate set). The scores are in the 100+ range (as we create a dot-product of 768 dimensional vectors, which naturally gives a larger score)\n",
        "\n",
        "*As a fun exercise you can swap the pre-trained model to the initial distilbert checkpoint and see that the example doesn't work anymore*\n",
        "\n",
        "- If you use our model checkpoint please cite our work as:\n",
        "\n",
        "    ```\n",
        "@inproceedings{Hofstaetter2021_tasb_dense_retrieval,\n",
        " author = {Sebastian Hofst{\\\"a}tter and Sheng-Chieh Lin and Jheng-Hong Yang and Jimmy Lin and Allan Hanbury},\n",
        " title = {{Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling}},\n",
        " booktitle = {Proc. of SIGIR},\n",
        " year = {2021},\n",
        "}\n",
        "    ```\n",
        "\n",
        "Thank You ðŸ˜Š If you have any questions feel free to reach out to Sebastian via mail (email in the paper). \n"
      ]
    }
  ]
}